{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import torch as T \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torchvision.transforms as trans \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/home/wwang/datasets/mini_imagenet/train/'\n",
    "path_val = '/home/wwang/datasets/mini_imagenet/val/'\n",
    "way = 5\n",
    "shot = 5\n",
    "image_size = [84, 84]\n",
    "dim_embedding = 256\n",
    "channel = 64\n",
    "iterations = 1000000\n",
    "batch_size = 32\n",
    "path_work = \"/home/wwang/wwfewshot/work/relationnet/\"\n",
    "device = T.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_siamesenet(Dataset):\n",
    "    #\n",
    "    def __init__(self, path):\n",
    "        category_names = os.listdir(path)\n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "        for label, category_name in enumerate(category_names):\n",
    "            file_names = os.listdir(path + category_name)\n",
    "            for file_name in file_names:\n",
    "                self.image_list.append(path + category_name + '/' + file_name)\n",
    "                self.label_list.append(label) \n",
    "        self.category_num = label + 1\n",
    "        self.transform = trans.Compose([trans.Resize(image_size, Image.BICUBIC),\n",
    "                                        trans.RandomHorizontalFlip(),\n",
    "                                        trans.ToTensor(),\n",
    "                                        trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "    #\n",
    "    def __getitem__(self, idx):\n",
    "        label = np.zeros([way])\n",
    "        permutation = np.random.permutation(self.category_num)[:way]\n",
    "        category_random = np.random.randint(way)\n",
    "        label[category_random] = 1\n",
    "        image_support = []\n",
    "        for i, cls in enumerate(permutation):\n",
    "            ids_per_cls = np.argwhere(np.array(self.label_list) == cls).reshape([-1])\n",
    "            ids_per_cls = np.random.choice(ids_per_cls, shot+1)\n",
    "            for j in range(shot):\n",
    "                image_support.append(self.transform(Image.open(self.image_list[ids_per_cls[j]])))\n",
    "            if i == category_random:\n",
    "                image_query = self.transform(Image.open(self.image_list[ids_per_cls[shot]]))\n",
    "        image_support = T.cat(image_support)\n",
    "        return image_support, image_query, label\n",
    "    #\n",
    "    def __len__(self):\n",
    "        return iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNA(nn.Module):\n",
    "    #\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackBone(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.layers = nn.Sequential(\n",
    "            CNA(3, channel, 5, 1, 0), # 84 to 80\n",
    "            CNA(channel, channel*2, 2, 2, 0), # 80 to 40\n",
    "            CNA(channel*2, channel*2, 5, 1, 0), # 40 to 36\n",
    "            CNA(channel*2, channel*4, 2, 2, 0), # 36 to 18\n",
    "            CNA(channel*4, channel*4, 5, 1, 0), # 18 to 14\n",
    "            CNA(channel*4, channel*8, 2, 2, 0), # 14 to 7\n",
    "            nn.Conv2d(channel*8, dim_embedding, 5, 1, 0) # 7 to 3\n",
    "        )\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationNet(nn.Module):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = BackBone(channel, dim_embedding)\n",
    "        self.layers = nn.Conv2d(dim_embedding*2, 1, 3, 1, 0)\n",
    "        # self.backbone = resnet18(pretrained=False, num_classes=dim_embedding)\n",
    "    #\n",
    "    def forward(self, image_support, image_query): # (batchsize, way*shot, 3, 224, 224) (batchsize, 3, 224, 224) ..(1, way) (1, way+1)\n",
    "        #\n",
    "        batch_size = image_support.shape[0]\n",
    "        image_support = image_support.reshape([batch_size*way*shot, 3, image_size[0], image_size[1]])\n",
    "        #\n",
    "        embedding_support = self.backbone(image_support)\n",
    "        embedding_query = self.backbone(image_query)\n",
    "        #\n",
    "        embedding_support = embedding_support.reshape([batch_size*way, shot, dim_embedding, 3, 3])\n",
    "        embedding_support_avg = T.mean(embedding_support, axis=1)\n",
    "        embedding_query = embedding_query.reshape([batch_size, dim_embedding, 3, 3])\n",
    "        embedding_query = embedding_query.repeat(way, 1, 1, 1)\n",
    "        embedding = T.cat([embedding_support_avg, embedding_query], axis=1)\n",
    "        #\n",
    "        score = self.layers(embedding).reshape(batch_size, way)\n",
    "        # score = F.softmax(score, 1)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@ T.no_grad()\n",
    "def show(net, path):\n",
    "    #\n",
    "    dataloader = DataLoader(Dataset_siamesenet(path=path), batch_size=1)\n",
    "    for image_support, image_query, label in dataloader:\n",
    "        break\n",
    "    image_support = image_support.reshape([-1, way*shot, 3, image_size[0], image_size[1]])\n",
    "    #\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    for i in range(way*shot+1):\n",
    "        plt.subplot(way+1, shot, i+1)\n",
    "        if i < way*shot:\n",
    "            image = image_support[0, i, :, :, :].cpu().numpy().transpose([1,2,0])\n",
    "            plt.title('Support. Label: {}'.format(int(label[0, i//shot])))\n",
    "        else:\n",
    "            image = image_query[0, :, :, :].cpu().numpy().transpose([1,2,0])\n",
    "            plt.title('Query')\n",
    "        plt.imshow(image*0.5+0.5)\n",
    "    plt.show()\n",
    "    #\n",
    "    if net is not None:\n",
    "        net.eval()\n",
    "        image_support, image_query = image_support.to(device), image_query.to(device)\n",
    "        score = net(image_support, image_query).softmax(1)\n",
    "        print('score: {}'.format(score.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RelationNet().to(device)\n",
    "show(net, path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m,h\n",
    "\n",
    "\n",
    "@ T.no_grad()\n",
    "def test(net, path, episode, batch_size=15):\n",
    "    net.eval()\n",
    "    dataloader = DataLoader(Dataset_siamesenet(path=path), batch_size=batch_size)\n",
    "    accuracies = []\n",
    "    for i, (image_support, image_query, label_relative) in enumerate(dataloader):\n",
    "        if i == episode:\n",
    "            break\n",
    "        image_support, image_query = image_support.to(device), image_query.to(device)\n",
    "        score = net(image_support, image_query)\n",
    "        accuracy = np.mean( score.cpu().numpy().argmax(1) == label_relative.numpy().argmax(1) )\n",
    "        accuracies.append(accuracy)\n",
    "    test_accuracy, h = mean_confidence_interval(accuracies)\n",
    "    return test_accuracy, h\n",
    "\n",
    "\n",
    "# net = RelationNet().to(device)\n",
    "# test_accuracy, h = test(net, path_val, episode=600, batch_size=15)\n",
    "# print(\"val accuracy:\", test_accuracy, \"h:\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(image_supportset, image_queryset, label_relative, net, optimizer):\n",
    "    #\n",
    "    y = net(image_supportset, image_queryset)\n",
    "    #\n",
    "    loss = F.cross_entropy(y, label_relative)\n",
    "    #\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(iterations, load_model):\n",
    "    #\n",
    "    iteration = np.load(path_work + 'iteration.npy', allow_pickle=True).item() if load_model else 0\n",
    "    print('Start training from iteration ', str(iteration))\n",
    "    net = RelationNet().to(device)\n",
    "    optimizer = T.optim.Adam(net.parameters(), 1e-4)\n",
    "    if load_model:\n",
    "        net.load_state_dict(T.load(path_work + 'net.pt'))\n",
    "        optimizer.load_state_dict(T.load(path_work + 'optimizer.pt'))\n",
    "    #\n",
    "    dataloader = DataLoader(Dataset_siamesenet(path=path_train), batch_size=batch_size)\n",
    "    time_start = time.time()\n",
    "    for image_support, image_query, label_relative in dataloader:\n",
    "        if iteration == iterations:\n",
    "            break\n",
    "        iteration += 1\n",
    "        image_support, image_query, label_relative = image_support.to(device), image_query.to(device), label_relative.to(device)\n",
    "        loss = trainer(image_support, image_query, label_relative, net, optimizer)\n",
    "    #\n",
    "        if(iteration % 100 == 0):\n",
    "            print('Iteration: {}, loss: {}'.format(iteration, loss.detach().cpu().numpy().item()))  \n",
    "            T.save(net.state_dict(), path_work + 'net.pt')\n",
    "            T.save(net.state_dict(), path_work + 'net_backup.pt')\n",
    "            T.save(optimizer.state_dict(), path_work + 'optimizer.pt')\n",
    "            T.save(optimizer.state_dict(), path_work + 'optimizer_backup.pt')\n",
    "            np.save(path_work + 'iteration.npy', iteration) \n",
    "            show(net, path_val)\n",
    "            print('These iterations cost {} seconds'.format(time.time() - time_start))\n",
    "            time_start = time.time()\n",
    "        if(iteration % 100000 == 0):\n",
    "            test_accuracy, h = test(net, path_val, episode=600, batchsize=15)\n",
    "            print(\"val accuracy:\", test_accuracy, \"h:\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train(iterations=1000, load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wwang/wwfewshot/Relation Network.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m train(iterations\u001b[39m=\u001b[39;49miterations, load_model\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/wwang/wwfewshot/Relation Network.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations, load_model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=10'>11</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(Dataset_siamesenet(path\u001b[39m=\u001b[39mpath_train), batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=11'>12</a>\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_support, image_query, label_relative \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m iteration \u001b[39m==\u001b[39m iterations:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=14'>15</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/wwang/wwfewshot/Relation Network.ipynb Cell 18\u001b[0m in \u001b[0;36mDataset_siamesenet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=27'>28</a>\u001b[0m ids_per_cls \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(ids_per_cls, shot\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(shot):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=29'>30</a>\u001b[0m     image_support\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(Image\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_list[ids_per_cls[j]])))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m category_random:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.56/home/wwang/wwfewshot/Relation%20Network.ipynb#ch0000017vscode-remote?line=31'>32</a>\u001b[0m     image_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_list[ids_per_cls[shot]]))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:98\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/functional.py:150\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(iterations=iterations, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = RelationNet().to(device)\n",
    "net.load_state_dict(T.load(path_work + 'net.pt'))\n",
    "show(net, path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_episode = 10\n",
    "total_accuracy = 0.0\n",
    "for _ in range(total_episode):\n",
    "    test_accuracy, h = test(net, path_val, episode=600, batch_size=15)\n",
    "    print(\"test accuracy:\", test_accuracy, \"h:\", h)\n",
    "    total_accuracy += test_accuracy\n",
    "print(\"aver_accuracy:\",total_accuracy/total_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cf6da5e8beced390049d9c6139c395b01a9f103347908593030ecc8eec04e13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
