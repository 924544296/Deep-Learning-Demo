{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch as T \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_label_train = '/home/wwang/datasets/celeba/list_attr_celeba.csv'\n",
    "path_label_test = '/home/wwang/datasets/celeba/test_list_attr_celeba.csv'\n",
    "path_image_train = '/home/wwang/datasets/celeba/img_align_celeba/'\n",
    "path_image_test = '/home/wwang/datasets/celeba/img_align_celeba/'\n",
    "crop_size = 178\n",
    "image_size = 128\n",
    "batch_size = 8 # 8\n",
    "channel_g = 64\n",
    "depth_g = 6\n",
    "depth_d = 5\n",
    "channel_d = 64\n",
    "learning_rate_g = 1e-4\n",
    "learning_rate_d = 1e-4\n",
    "λ_src = 1\n",
    "λ_cls = 1\n",
    "λ_gp = 10\n",
    "λ_rec = 10\n",
    "step_g = 5\n",
    "path_work = \"/home/wwang/wwgeneration/work/StarGAN/\"\n",
    "iterations = 202599 * 200\n",
    "num_workers = 4\n",
    "device = T.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_stargan(Dataset):\n",
    "    #\n",
    "    def __init__(self, path_label=path_label_train, path_image=path_image_train):\n",
    "        with open(path_label) as f:\n",
    "            self.list_ = f.readlines()\n",
    "        self.path_image = path_image\n",
    "        self.transform = trans.Compose([trans.CenterCrop(crop_size),\n",
    "                                        trans.Resize(image_size, Image.BICUBIC),\n",
    "                                        trans.RandomHorizontalFlip(),\n",
    "                                        trans.ToTensor(),\n",
    "                                        trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "    #\n",
    "    def load_image_and_label(self, list_, path_image):\n",
    "        idx_ = np.random.randint(len(list_))\n",
    "        line = list_[idx_].replace('\\n','').split(',')\n",
    "        image = Image.open(path_image + line[0])\n",
    "        image = self.transform(image)\n",
    "        label = T.from_numpy(np.array([line[9], line[10], line[12], line[21], line[40]], dtype='float32').reshape(5,1,1)) * 0.5 + 0.5\n",
    "        return image, label\n",
    "    #\n",
    "    def __getitem__(self, idx):\n",
    "        image_a, label_a = self.load_image_and_label(self.list_, self.path_image)\n",
    "        image_b, label_b = self.load_image_and_label(self.list_, self.path_image)\n",
    "        return image_a, label_a, image_b, label_b\n",
    "    #\n",
    "    def __len__(self):\n",
    "        return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(Dataset_stargan(), batch_size=1)\n",
    "n = 0\n",
    "for image_a, label_a, image_b, label_b in dataloader:\n",
    "    if n == 2:\n",
    "        break\n",
    "    image_a = image_a.squeeze().numpy().transpose([1,2,0]) * 0.5 + 0.5\n",
    "    image_b = image_b.squeeze().numpy().transpose([1,2,0]) * 0.5 + 0.5\n",
    "    plt.subplot(1,2,1), plt.imshow(image_a), plt.title(np.squeeze(label_a.numpy()))\n",
    "    plt.subplot(1,2,2), plt.imshow(image_b), plt.title(np.squeeze(label_b.numpy()))\n",
    "    plt.show()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel, channel, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(channel)\n",
    "        )\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNA(nn.Module):\n",
    "    #\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_conv=True):\n",
    "        super().__init__()\n",
    "        if is_conv:\n",
    "            self.layers = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)])\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)])\n",
    "        self.layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        self.layers.append(nn.ReLU())\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel=channel_g, depth=depth_g):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CNA(3+5, channel, 7, 1, 3, True)])\n",
    "        self.layers.append(CNA(channel, channel*2, 4, 2, 1, True))\n",
    "        self.layers.append(CNA(channel*2, channel*4, 4, 2, 1, True))\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(ResBlock(channel*4))\n",
    "        self.layers.append(CNA(channel*4, channel*2, 4, 2, 1, False))\n",
    "        self.layers.append(CNA(channel*2, channel, 4, 2, 1, False))\n",
    "        self.layers.append(nn.Conv2d(channel, 3, 7, 1, 3, bias=False))\n",
    "    #\n",
    "    def forward(self, image, label):\n",
    "        label = label.repeat(1,1,image.shape[2],image.shape[3]).to(device)\n",
    "        x = T.cat([image, label], 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel=channel_d, depth=depth_d):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, channel, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU()\n",
    "        ])\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.Conv2d(channel*2**i, channel*2**(i+1), 4, 2, 1, bias=False))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "        self.D_src = nn.Conv2d(channel*2**(i+1), 1, 3, 1, 1, bias=False)\n",
    "        self.D_cls = nn.Conv2d(channel*2**(i+1), 5, [image_size//2**(1+depth), image_size//2**(1+depth)], 1, 0, bias=False)\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        src = self.D_src(x)\n",
    "        cls = self.D_cls(x)\n",
    "        return src, cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@T.no_grad()\n",
    "def show(GG=None):\n",
    "    if GG==None:\n",
    "        GG = Generator().to(device)\n",
    "    GG.eval()\n",
    "    fig = plt.figure(figsize=(26, 3))\n",
    "    gs = plt.GridSpec(1, 9)\n",
    "    gs.update(wspace=0.01, hspace=0.01)\n",
    "    image0 = Image.open('/home/wwang/datasets/celeba/img_align_celeba/002044.jpg')#002044, 000077\n",
    "    image0 = image0.crop([0,20,178,198])\n",
    "    image0 = image0.resize((128,128),Image.BICUBIC)\n",
    "    image0 = np.array(image0).astype(\"float32\").transpose([2, 0, 1]).reshape(1,3,128,128) / 255 * 2 - 1\n",
    "    images0 = np.tile(image0,(8,1,1,1))\n",
    "    att1, att2, att3, att4, att5 = 0, 0, 1, 0, 1 #棕发、女、年轻 \n",
    "    att_list = ['brown, female, young', 'black, female, young', 'blond, female, young', 'brown, male, young', 'brown, female, old', 'black, male, young', 'black, female, old', 'brown, male, old', 'black, male, old']\n",
    "    labels = np.empty([8,5,1,1], dtype = 'float32')\n",
    "    labels[0,:,:,:] = np.array([[[[1-att1]], [[att2]],[[1-att3]],[[att4]],[[att5]]]]) #黑发、女、年轻\n",
    "    labels[1,:,:,:] = np.array([[[[att1]], [[1-att2]],[[1-att3]],[[att4]],[[att5]]]]) #金发、女、年轻\n",
    "    labels[2,:,:,:] = np.array([[[[att1]], [[att2]],[[att3]],[[1-att4]],[[att5]]]]) #棕发、男、年轻\n",
    "    labels[3,:,:,:] = np.array([[[[att1]], [[att2]],[[att3]],[[att4]],[[1-att5]]]]) #棕发、女、年老\n",
    "    labels[4,:,:,:] = np.array([[[[1-att1]], [[att2]],[[1-att3]],[[1-att4]],[[att5]]]]) #黑发、男、年轻\n",
    "    labels[5,:,:,:] = np.array([[[[1-att1]], [[att2]],[[1-att3]],[[att4]],[[1-att5]]]]) #黑发、女、年老\n",
    "    labels[6,:,:,:] = np.array([[[[att1]], [[att2]],[[att3]],[[1-att4]],[[1-att5]]]]) #棕发、男、年老\n",
    "    labels[7,:,:,:] = np.array([[[[1-att1]], [[att2]],[[1-att3]],[[1-att4]],[[1-att5]]]]) #黑发、男、年老\n",
    "    images998 = GG(T.from_numpy(images0).to(device), T.from_numpy(labels.astype('float32'))).cpu().numpy()\n",
    "    images998 = np.concatenate((image0, images998),axis = 0)\n",
    "    n = 0\n",
    "    for j in range(9):\n",
    "        ax = plt.subplot(gs[n])\n",
    "        n += 1\n",
    "        image = (images998[j,:,:,:].reshape(3,128,128).transpose(1,2,0)+1)/2\n",
    "        image = np.maximum(image, 0)\n",
    "        image = np.minimum(image, 1)\n",
    "        # image = Image.fromarray(np.uint8(image*255))\n",
    "        plt.imshow(image)  \n",
    "        plt.title(att_list[j])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(y, x):\n",
    "    \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "    weight = T.ones(y.shape).to(device)\n",
    "    dydx = T.autograd.grad(y, x, grad_outputs=weight, retain_graph=True, create_graph=True)[0]\n",
    "    dydx = dydx.reshape([dydx.shape[0], -1])\n",
    "    dydx_l2norm = T.sqrt(T.sum(dydx**2, axis=1))\n",
    "    return T.mean((dydx_l2norm-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainer_d(image_a, image_b, label_b, net_g, net_d, optimizer_d):\n",
    "    #\n",
    "    image_a2b = net_g(image_a, label_b)\n",
    "    src_b, cls_b = net_d(image_b)\n",
    "    src_a2b, _ = net_d(image_a2b.detach())\n",
    "    # gp\n",
    "    α = T.rand([label_b.shape[0], 1, 1, 1]).to(device)\n",
    "    image_fusion = α*image_b + (1-α)*image_a2b\n",
    "    src_fusion, _ = net_d(image_fusion)\n",
    "    #\n",
    "    loss_src = -T.mean(src_b) + T.mean(src_a2b)\n",
    "    # loss_cls = P.mean(-label_b*P.log(F.sigmoid(cls_b)+1e-8) - (1-label_b)*P.log(1-F.sigmoid(cls_b)+1e-8)) \n",
    "    loss_cls = F.binary_cross_entropy_with_logits(cls_b, label_b)\n",
    "    loss_gp = gradient_penalty(src_fusion, image_fusion)\n",
    "    loss = λ_src*loss_src + λ_cls*loss_cls + λ_gp*loss_gp\n",
    "    #\n",
    "    # net_d.zero_gradients()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_d.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainer_g(image_a, label_a, label_b, net_g, net_d, optimizer_g):\n",
    "    #\n",
    "    image_a2b = net_g(image_a, label_b)\n",
    "    src_a2b, cls_a2b = net_d(image_a2b)\n",
    "    image_a2b2a = net_g(image_a2b, label_a)\n",
    "    #\n",
    "    loss_src = -T.mean(src_a2b)\n",
    "    loss_rec = T.mean(T.abs(image_a-image_a2b2a))\n",
    "    # loss_cls = P.mean(-label_b*P.log(F.sigmoid(cls_a2b)+1e-8)-(1-label_b)*P.log(1-F.sigmoid(cls_a2b)+1e-8))\n",
    "    loss_cls = F.binary_cross_entropy_with_logits(cls_a2b, label_b)\n",
    "    loss = λ_src*loss_src + λ_rec*loss_rec + λ_cls*loss_cls\n",
    "    #\n",
    "    optimizer_g.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_g.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(iteration=1000000, load_model=False):\n",
    "    #\n",
    "    iteration_ = np.load(path_work + 'iteration.npy', allow_pickle=True).item() if load_model else -1\n",
    "    net_g = Generator().to(device)\n",
    "    net_d = Discriminator().to(device)\n",
    "    # net_d.train()\n",
    "    optimizer_g = T.optim.Adam(net_g.parameters(), learning_rate_g, betas=(0.5,0.999))\n",
    "    optimizer_d = T.optim.Adam(net_d.parameters(), learning_rate_d, betas=(0.5,0.999))\n",
    "    if load_model:\n",
    "        net_g.load_state_dict(T.load(path_work + 'net_g.pt'))\n",
    "        optimizer_g.load_state_dict(T.load(path_work + 'optimizer_g.pt'))\n",
    "        net_d.load_state_dict(T.load(path_work + 'net_d.pt'))\n",
    "        optimizer_d.load_state_dict(T.load(path_work + 'optimizer_d.pt'))\n",
    "    #\n",
    "    time_start = time.time()\n",
    "    dataloader = DataLoader(Dataset_stargan(), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    for image_a, label_a, image_b, label_b in dataloader:\n",
    "        if iteration_ >= iteration:\n",
    "            break\n",
    "        iteration_ += 1\n",
    "        net_g.train()        \n",
    "        image_a, image_b, label_a, label_b = image_a.to(device), image_b.to(device), label_a.to(device), label_b.to(device)\n",
    "        trainer_d(image_a, image_b, label_b, net_g, net_d, optimizer_d)\n",
    "        if iteration_ % step_g == 0:\n",
    "            trainer_g(image_a, label_a, label_b, net_g, net_d, optimizer_g)\n",
    "    #\n",
    "        if iteration_ % 1000 == 0:\n",
    "            print('Iteration: ', iteration_)    \n",
    "            show(net_g)        \n",
    "            T.save(net_d.state_dict(), path_work + 'net_d.pt')\n",
    "            T.save(net_d.state_dict(), path_work + '备用net_d.pt')\n",
    "            T.save(optimizer_d.state_dict(), path_work + 'optimizer_d.pt')\n",
    "            T.save(optimizer_d.state_dict(), path_work + '备用optimizer_d.pt')\n",
    "            T.save(net_g.state_dict(), path_work + 'net_g.pt')\n",
    "            T.save(net_g.state_dict(), path_work + '备用net_g.pt')\n",
    "            T.save(optimizer_g.state_dict(), path_work + 'optimizer_g.pt')\n",
    "            T.save(optimizer_g.state_dict(), path_work + '备用optimizer_g.pt')\n",
    "            np.save(path_work + 'iteration.npy', iteration_)\n",
    "            print('These iterations cost {} seconds'.format(time.time() - time_start))\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train(iteration=1, load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(iteration=iterations, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_g = Generator().to(device)\n",
    "net_g.load_state_dict(T.load(path_work + 'net_g.pt'))\n",
    "show(net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cf6da5e8beced390049d9c6139c395b01a9f103347908593030ecc8eec04e13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
