{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:02.344487Z",
     "iopub.status.busy": "2022-06-23T11:30:02.343926Z",
     "iopub.status.idle": "2022-06-23T11:30:04.036740Z",
     "shell.execute_reply": "2022-06-23T11:30:04.036058Z",
     "shell.execute_reply.started": "2022-06-23T11:30:02.344460Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "source": [
    "数据集来源：使用make_data_dota.py得到DOTA中含有ship的图像。本项目Dataset_cyclegan利用标签把可见光中的ship替换红外ship（背景保持不变），使构建的训练图像对中两张图的差别仅是标签处的船不同。与v3的区别是抠掉的位置不一定在红外的标签处，而是随机  \n",
    "修改Dataset中的image_b，是否配对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "from PIL import Image, ImageFilter \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:04.038198Z",
     "iopub.status.busy": "2022-06-23T11:30:04.037713Z",
     "iopub.status.idle": "2022-06-23T11:30:04.043163Z",
     "shell.execute_reply": "2022-06-23T11:30:04.042624Z",
     "shell.execute_reply.started": "2022-06-23T11:30:04.038169Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data \n",
    "path_a_line = [\"/home/wwang/datasets/ship_line/\"]\n",
    "path_a_background = [\"/home/wwang/datasets/ship_background/\"]\n",
    "path_b = [\"/home/wwang/datasets/ship/data/images/\"]\n",
    "image_size = [256, 256]\n",
    "# network\n",
    "channel_d = 64\n",
    "channel_g = 64\n",
    "layer_resblock = 9\n",
    "# training\n",
    "λ_cycle_consistency = 10 # 10\n",
    "λ_identity = 5 # 5\n",
    "batch_size = 1 # 1\n",
    "num_workers = 4\n",
    "iterations = 10000000\n",
    "learning_rate_d1 = 1e-4 # 2e-4\n",
    "learning_rate_g1 = 1e-4 # 2e-4\n",
    "learning_rate_d2 = 1e-4 # 2e-4\n",
    "learning_rate_g2 = 1e-4 # 2e-4\n",
    "apply_train_combined = False # True \n",
    "apply_contrast_loss = False # False \n",
    "apply_identity_loss = True # True \n",
    "step_g = 1 # 1\n",
    "pool_size = 50\n",
    "# \n",
    "device = T.device(\"cuda:0\")\n",
    "path_work = \"/home/wwang/wwgeneration/work/CycleGAN_v10/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:04.044201Z",
     "iopub.status.busy": "2022-06-23T11:30:04.043924Z",
     "iopub.status.idle": "2022-06-23T11:30:04.050602Z",
     "shell.execute_reply": "2022-06-23T11:30:04.050045Z",
     "shell.execute_reply.started": "2022-06-23T11:30:04.044167Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_dir(path):\n",
    "    list_ = []\n",
    "    for path_ in path:\n",
    "        dirs = os.listdir(path_)\n",
    "        dirs.sort()\n",
    "        for dir_ in dirs:\n",
    "            list_.append(path_ + dir_)\n",
    "    return list_\n",
    "\n",
    "\n",
    "list_a_line = path_to_dir(path_a_line)\n",
    "list_a_background = path_to_dir(path_a_background)\n",
    "list_b = path_to_dir(path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_cyclegan(Dataset):\n",
    "    #\n",
    "    def __init__(self, list_a_line=list_a_line, list_a_background=list_a_background, list_b=list_b):\n",
    "        self.list_a_line = list_a_line\n",
    "        self.list_a_background = list_a_background\n",
    "        self.list_b = list_b\n",
    "        self.transform = trans.Compose([trans.ToTensor(),\n",
    "                                        trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "    #\n",
    "    def augmentation_image(self, image):\n",
    "        if np.random.randint(2):\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT) \n",
    "        n = np.random.randint(4)\n",
    "        if n == 1:\n",
    "            image = image.rotate(90, expand=1)\n",
    "        if n == 2:\n",
    "            image = image.rotate(180, expand=1)\n",
    "        if n == 3:\n",
    "            image = image.rotate(270, expand=1)\n",
    "        return image\n",
    "    #\n",
    "    def augmentation_images(self, a, b):\n",
    "        if np.random.randint(2):\n",
    "            a = a.transpose(Image.FLIP_LEFT_RIGHT) \n",
    "            b = b.transpose(Image.FLIP_LEFT_RIGHT) \n",
    "        n = np.random.randint(4)\n",
    "        if n == 1:\n",
    "            a = a.rotate(90, expand=1)\n",
    "            b = b.rotate(90, expand=1)\n",
    "        if n == 2:\n",
    "            a = a.rotate(180, expand=1)\n",
    "            b = b.rotate(180, expand=1)\n",
    "        if n == 3:\n",
    "            a = a.rotate(270, expand=1)\n",
    "            b = b.rotate(270, expand=1)\n",
    "        return a, b\n",
    "    #\n",
    "    def load_imageb(self):\n",
    "        idx_b = np.random.randint(len(self.list_b))\n",
    "        image = Image.open(self.list_b[idx_b]).convert('RGB').resize(image_size)\n",
    "        image = self.augmentation_image(image)\n",
    "        return image\n",
    "    #\n",
    "    def make_images(self):\n",
    "        #\n",
    "        idx_b = np.random.randint(len(self.list_b))\n",
    "        image_b = Image.open(self.list_b[idx_b]).convert('RGB').resize(image_size)\n",
    "        # image = self.augmentation_image(image)\n",
    "        #\n",
    "        idx_line = np.random.randint(len(self.list_a_line))\n",
    "        image_line = Image.open(self.list_a_line[idx_line])\n",
    "        image_line = self.augmentation_image(image_line)\n",
    "        #\n",
    "        image_background = Image.open(self.list_a_background[idx_b]).convert('RGB').resize(image_size)\n",
    "        # image_background = self.augmentation_image(image_background)\n",
    "        #\n",
    "        image_background, image_b = self.augmentation_images(image_background, image_b)\n",
    "        #\n",
    "        w, h = image_background.size\n",
    "        w_patch, h_patch = np.random.randint(image_size[1]//8, image_size[1]+1), np.random.randint(image_size[0]//8, image_size[0]+1)\n",
    "        w_random, h_random = np.random.randint(w-w_patch+1), np.random.randint(h-h_patch+1)\n",
    "        image_background.paste(image_line.resize([w_patch, h_patch]), [w_random, h_random])\n",
    "        return image_background, image_b\n",
    "    #\n",
    "    def __getitem__(self, idx):\n",
    "        # image_a, image_b = self.make_images()\n",
    "        image_a, _ = self.make_images()\n",
    "        image_b = self.load_imageb() # \n",
    "        image_a = self.transform(image_a)\n",
    "        image_b = self.transform(image_b)\n",
    "        return image_a, image_b\n",
    "    #\n",
    "    def __len__(self):\n",
    "        return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:04.059461Z",
     "iopub.status.busy": "2022-06-23T11:30:04.059308Z",
     "iopub.status.idle": "2022-06-23T11:30:05.686426Z",
     "shell.execute_reply": "2022-06-23T11:30:05.685845Z",
     "shell.execute_reply.started": "2022-06-23T11:30:04.059443Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(Dataset_cyclegan(), batch_size=1)\n",
    "n = 0\n",
    "for image_a, image_b in dataloader:\n",
    "    if n == 2:\n",
    "        break\n",
    "    image_a = image_a.numpy().squeeze().transpose([1,2,0])\n",
    "    image_b = image_b.numpy().squeeze().transpose([1,2,0])\n",
    "    plt.subplot(1,2,1), plt.imshow(image_a/2+0.5), plt.title('a')\n",
    "    plt.subplot(1,2,2), plt.imshow(image_b/2+0.5), plt.title('b')\n",
    "    plt.show()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.687830Z",
     "iopub.status.busy": "2022-06-23T11:30:05.687397Z",
     "iopub.status.idle": "2022-06-23T11:30:05.694756Z",
     "shell.execute_reply": "2022-06-23T11:30:05.694275Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.687803Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNA(nn.Module):\n",
    "    #\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, conv='conv', norm='in', act='relu'):\n",
    "        super().__init__()\n",
    "        if conv == 'conv':\n",
    "            self.layers = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)])\n",
    "        if conv == 'tconv':\n",
    "            self.layers = nn.ModuleList([nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)])\n",
    "        if norm == 'in':\n",
    "            self.layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        if act == 'relu':\n",
    "            self.layers.append(nn.ReLU())\n",
    "        if act == 'lrelu':\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.695879Z",
     "iopub.status.busy": "2022-06-23T11:30:05.695531Z",
     "iopub.status.idle": "2022-06-23T11:30:05.701608Z",
     "shell.execute_reply": "2022-06-23T11:30:05.701106Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.695855Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel=channel_d):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        channels = [3, channel, channel*2, channel*4, channel*8]\n",
    "        strides = [2, 2, 2, 1]\n",
    "        norms = [False, 'in', 'in', 'in']\n",
    "        for i in range(4):\n",
    "            self.layers.append(CNA(channels[i], channels[i+1], 4, stride=strides[i], padding=1, conv='conv', norm=norms[i], act='lrelu'))\n",
    "        self.layers.append(nn.Conv2d(channels[i+1], 1, 4, stride=1, padding=1))\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.702485Z",
     "iopub.status.busy": "2022-06-23T11:30:05.702330Z",
     "iopub.status.idle": "2022-06-23T11:30:05.707363Z",
     "shell.execute_reply": "2022-06-23T11:30:05.706877Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.702465Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, 3, 1, 1, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel, channel, 3, 1, 1, padding_mode='reflect', bias=False),\n",
    "            nn.InstanceNorm2d(channel))\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.708310Z",
     "iopub.status.busy": "2022-06-23T11:30:05.708083Z",
     "iopub.status.idle": "2022-06-23T11:30:05.716041Z",
     "shell.execute_reply": "2022-06-23T11:30:05.715547Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.708288Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #\n",
    "    def __init__(self, channel=channel_g, layer_resblock=layer_resblock):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        channels = [3, channel, channel*2, channel*4]\n",
    "        kernel_sizes = [7, 4, 4]\n",
    "        strides = [1, 2, 2]\n",
    "        paddings = [3, 1, 1]\n",
    "        for i in range(3):\n",
    "            self.layers.append(CNA(channels[i], channels[i+1], kernel_sizes[i], strides[i], paddings[i], 'conv', 'in', act='relu'))\n",
    "        for _ in range(layer_resblock):\n",
    "            self.layers.append(ResBlock(channels[-1]))\n",
    "        for i in range(2):\n",
    "            self.layers.append(CNA(channels[-i-1], channels[-i-2], kernel_sizes[-i-1], strides[-i-1], paddings[-i-1], 'tconv', 'in', 'relu'))\n",
    "        self.layers.append(nn.ConvTranspose2d(channels[1], channels[0], kernel_size=7, stride=1, padding=3, bias=False))\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T09:17:01.744713Z",
     "iopub.status.busy": "2022-06-13T09:17:01.743802Z",
     "iopub.status.idle": "2022-06-13T09:17:01.748948Z",
     "shell.execute_reply": "2022-06-13T09:17:01.748108Z",
     "shell.execute_reply.started": "2022-06-13T09:17:01.744642Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.718401Z",
     "iopub.status.busy": "2022-06-23T11:30:05.717974Z",
     "iopub.status.idle": "2022-06-23T11:30:05.724751Z",
     "shell.execute_reply": "2022-06-23T11:30:05.724250Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.718377Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_translation(image_a, G1, G2):\n",
    "    #\n",
    "    # image_a = image.transpose([1,2,0]) * 0.5 + 0.5\n",
    "    # x = T(image_a[None,:,:,:] * 2 - 1).transpose([0,3,1,2])\n",
    "    #\n",
    "    image_a2b = G1(image_a)\n",
    "    image_a2b2a = G2(image_a2b)\n",
    "    image_a = image_a.squeeze().cpu().numpy().transpose([1,2,0]) / 2 + 0.5\n",
    "    image_a2b = image_a2b.squeeze().cpu().numpy().transpose([1,2,0]) / 2 + 0.5\n",
    "    image_a2b2a = image_a2b2a.squeeze().cpu().numpy().transpose([1,2,0]) / 2 + 0.5\n",
    "    #\n",
    "    plt.figure(figsize=[20,20])\n",
    "    plt.subplot(1,3,1), plt.imshow(image_a), plt.title('a')\n",
    "    plt.subplot(1,3,2), plt.imshow(image_a2b), plt.title('a2b')\n",
    "    plt.subplot(1,3,3), plt.imshow(image_a2b2a), plt.title('a2b2a')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.725887Z",
     "iopub.status.busy": "2022-06-23T11:30:05.725643Z",
     "iopub.status.idle": "2022-06-23T11:30:05.730766Z",
     "shell.execute_reply": "2022-06-23T11:30:05.730285Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.725865Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@T.no_grad()\n",
    "def show(G1=None, G2=None):\n",
    "    #\n",
    "    if G1 is None:\n",
    "        G1 = Generator().to(device)\n",
    "    G1.eval()\n",
    "    if G2 is None:\n",
    "        G2 = Generator().to(device)\n",
    "    G2.eval()\n",
    "    #\n",
    "    dataloader = DataLoader(Dataset_cyclegan(), batch_size=1)\n",
    "    for image_a, image_b in dataloader:\n",
    "        image_a, image_b = image_a.to(device), image_b.to(device)\n",
    "        image_translation(image_a, G1, G2)\n",
    "        image_translation(image_b, G2, G1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:05.731864Z",
     "iopub.status.busy": "2022-06-23T11:30:05.731471Z",
     "iopub.status.idle": "2022-06-23T11:30:11.133692Z",
     "shell.execute_reply": "2022-06-23T11:30:11.133111Z",
     "shell.execute_reply.started": "2022-06-23T11:30:05.731831Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.135160Z",
     "iopub.status.busy": "2022-06-23T11:30:11.134706Z",
     "iopub.status.idle": "2022-06-23T11:30:11.139968Z",
     "shell.execute_reply": "2022-06-23T11:30:11.139481Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.135131Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    #\n",
    "    def __init__(self, pool_size=pool_size):\n",
    "        self.pool = []\n",
    "        self.count = 0\n",
    "        self.pool_size = pool_size\n",
    "    #\n",
    "    def pool_image(self, image):\n",
    "        if self.count < self.pool_size:\n",
    "            self.pool.append(image.cpu().numpy())\n",
    "            self.count += 1\n",
    "            return image\n",
    "        else:\n",
    "            if np.random.randint(2):\n",
    "                return image\n",
    "            else:\n",
    "                random_id = np.random.randint(self.pool_size)\n",
    "                temp = self.pool[random_id]\n",
    "                self.pool[random_id] = image.cpu().numpy()\n",
    "                return T.from_numpy(temp).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.141086Z",
     "iopub.status.busy": "2022-06-23T11:30:11.140849Z",
     "iopub.status.idle": "2022-06-23T11:30:11.145294Z",
     "shell.execute_reply": "2022-06-23T11:30:11.144796Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.141063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool_a2b, pool_b2a = ImagePool(), ImagePool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.146622Z",
     "iopub.status.busy": "2022-06-23T11:30:11.146346Z",
     "iopub.status.idle": "2022-06-23T11:30:11.156446Z",
     "shell.execute_reply": "2022-06-23T11:30:11.155945Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.146599Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(net_g1, net_d1, net_g2, net_d2, optimizer_g1, optimizer_d1, optimizer_g2, optimizer_d2, image_a, image_b, apply_identity_loss=apply_identity_loss, apply_contrast_loss=apply_contrast_loss):\n",
    "    # outputs of generators\n",
    "    image_a2b = net_g1(image_a)\n",
    "    image_a2b2a = net_g2(image_a2b)\n",
    "    image_b2a = net_g2(image_b)\n",
    "    image_b2a2b = net_g1(image_b2a)\n",
    "    if apply_identity_loss:\n",
    "        image_a2a = net_g2(image_a)\n",
    "        image_b2b = net_g1(image_b)\n",
    "    # outputs of discriminators\n",
    "    d_a2b = net_d1(pool_a2b.pool_image(image_a2b))\n",
    "    d_b = net_d1(image_b)\n",
    "    d_b2a = net_d2(pool_b2a.pool_image(image_b2a))\n",
    "    d_a = net_d2(image_a)\n",
    "    if apply_contrast_loss:\n",
    "        d_a = net_d1(image_a)\n",
    "        d_b = net_d2(image_b)\n",
    "    # loss d1\n",
    "    loss_d1 = T.mean(d_a2b**2) + T.mean((d_b-1)**2)\n",
    "    if apply_contrast_loss:\n",
    "        loss_d1 += T.mean(d_a**2)\n",
    "        loss_d1 /= 3\n",
    "    else:\n",
    "        loss_d1 /= 2\n",
    "    # loss g1\n",
    "    loss_g1 = T.mean((d_a2b-1)**2) + (T.mean(T.abs(image_a-image_a2b2a)) + T.mean(T.abs(image_b-image_b2a2b))) * λ_cycle_consistency\n",
    "    if apply_identity_loss:\n",
    "        loss_g1 += T.mean(T.abs(image_b-image_b2b)) * λ_identity\n",
    "    # loss d2\n",
    "    loss_d2 = T.mean(d_b2a**2) + T.mean((d_a-1)**2)\n",
    "    if apply_contrast_loss:\n",
    "        loss_d2 += T.mean(d_b**2)\n",
    "        loss_d2 /= 3\n",
    "    else:\n",
    "        loss_d2 /= 2\n",
    "    # loss g2\n",
    "    loss_g2 = T.mean((d_b2a-1)**2) + (T.mean(T.abs(image_b-image_b2a2b)) + T.mean(T.abs(image_a-image_a2b2a))) * λ_cycle_consistency\n",
    "    if apply_identity_loss:\n",
    "        loss_g2 += T.mean(T.abs(image_a-image_a2a)) * λ_identity\n",
    "    # optimizer d1\n",
    "    optimizer_d1.zero_grad()\n",
    "    loss_d1.backward(retain_graph=True)\n",
    "    optimizer_d1.step()\n",
    "    # optimizer g1\n",
    "    optimizer_g1.zero_grad()\n",
    "    loss_g1.backward(retain_graph=True)\n",
    "    optimizer_g1.step()\n",
    "    # optimizer d2\n",
    "    optimizer_d2.zero_grad()\n",
    "    loss_d2.backward(retain_graph=True)\n",
    "    optimizer_d2.step()\n",
    "    # optimizer g2\n",
    "    optimizer_g2.zero_grad()\n",
    "    loss_g2.backward()\n",
    "    optimizer_g2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.157415Z",
     "iopub.status.busy": "2022-06-23T11:30:11.157182Z",
     "iopub.status.idle": "2022-06-23T11:30:11.162182Z",
     "shell.execute_reply": "2022-06-23T11:30:11.161692Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.157393Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainer_d(net_g, net_d, optimizer_d, image_a, image_b, pool, apply_contrast_loss=apply_contrast_loss):\n",
    "    # outputs of generator\n",
    "    image_a2b = net_g(image_a)\n",
    "    # outputs of discriminator\n",
    "    d_a2b = net_d(pool.pool_image(image_a2b.detach()))\n",
    "    d_b = net_d(image_b)\n",
    "    if apply_contrast_loss:\n",
    "        d_a = net_d(image_a)\n",
    "    # loss d\n",
    "    loss_d = T.mean(d_a2b**2) + T.mean((d_b-1)**2)\n",
    "    if apply_contrast_loss:\n",
    "        loss_d += T.mean(d_a**2)\n",
    "        loss_d /= 3\n",
    "    else:\n",
    "        loss_d /= 2\n",
    "    # optimizer d\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizer_d.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.163230Z",
     "iopub.status.busy": "2022-06-23T11:30:11.162898Z",
     "iopub.status.idle": "2022-06-23T11:30:11.167990Z",
     "shell.execute_reply": "2022-06-23T11:30:11.167525Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.163208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainer_g(net_g1, net_d1, net_g2, optimizer_g1, image_a, image_b, apply_cycle_loss):\n",
    "    # outputs of generators\n",
    "    image_a2b = net_g1(image_a)\n",
    "    image_b2a = net_g2(image_b)\n",
    "    if apply_cycle_loss:\n",
    "        image_a2b2a = net_g2(image_a2b)\n",
    "        image_b2a2b = net_g1(image_b2a)\n",
    "    if apply_identity_loss:\n",
    "        image_b2b = net_g1(image_b)\n",
    "    # outputs of discriminators\n",
    "    d_a2b = net_d1(image_a2b)\n",
    "    # loss g1\n",
    "    loss_g1 = T.mean((d_a2b-1)**2)\n",
    "    if apply_cycle_loss:\n",
    "        loss_g1 += (T.mean(T.abs(image_a-image_a2b2a)) + T.mean(T.abs(image_b-image_b2a2b))) * λ_cycle_consistency\n",
    "    if apply_identity_loss:\n",
    "        loss_g1 += T.mean(T.abs(image_b-image_b2b)) * λ_identity\n",
    "    # optimizer g1\n",
    "    optimizer_g1.zero_grad()\n",
    "    loss_g1.backward()\n",
    "    optimizer_g1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.169032Z",
     "iopub.status.busy": "2022-06-23T11:30:11.168875Z",
     "iopub.status.idle": "2022-06-23T11:30:11.182059Z",
     "shell.execute_reply": "2022-06-23T11:30:11.181585Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.169013Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(iterations, load_model):\n",
    "    #\n",
    "    iteration = np.load(path_work + 'iteration.npy', allow_pickle=True).item() if load_model else -1\n",
    "    print('Start training from iteration ', str(iteration))\n",
    "    net_g1 = Generator().to(device)\n",
    "    net_d1 = Discriminator().to(device)\n",
    "    net_g2 = Generator().to(device)\n",
    "    net_d2 = Discriminator().to(device)\n",
    "    net_d1.train()\n",
    "    net_d2.train()\n",
    "    optimizer_g1 = T.optim.Adam(net_g1.parameters(), learning_rate_g1, betas=(0.5, 0.999))\n",
    "    optimizer_d1 = T.optim.Adam(net_d1.parameters(), learning_rate_d1, betas=(0.5, 0.999))\n",
    "    optimizer_g2 = T.optim.Adam(net_g2.parameters(), learning_rate_g2, betas=(0.5, 0.999))\n",
    "    optimizer_d2 = T.optim.Adam(net_d2.parameters(), learning_rate_d2, betas=(0.5, 0.999))\n",
    "    if load_model:\n",
    "        net_g1.load_state_dict(T.load(path_work + 'net_g1.pt'))\n",
    "        optimizer_g1.load_state_dict(T.load(path_work + 'optimizer_g1.pt'))\n",
    "        net_d1.load_state_dict(T.load(path_work + 'net_d1.pt'))\n",
    "        optimizer_d1.load_state_dict(T.load(path_work + 'optimizer_d1.pt'))\n",
    "        net_g2.load_state_dict(T.load(path_work + 'net_g2.pt'))\n",
    "        optimizer_g2.load_state_dict(T.load(path_work + 'optimizer_g2.pt'))\n",
    "        net_d2.load_state_dict(T.load(path_work + 'net_d2.pt'))\n",
    "        optimizer_d2.load_state_dict(T.load(path_work + 'optimizer_d2.pt'))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    dataloader = DataLoader(Dataset_cyclegan(), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    for image_a, image_b in dataloader:\n",
    "        if iteration >= iterations:\n",
    "            break\n",
    "        iteration += 1\n",
    "        image_a, image_b = image_a.to(device), image_b.to(device)\n",
    "        if apply_train_combined:\n",
    "            net_g1.train()\n",
    "            net_g2.train()  \n",
    "            trainer(net_g1, net_d1, net_g2, net_d2, optimizer_g1, optimizer_d1, optimizer_g2, optimizer_d2, image_a, image_b)\n",
    "        else:\n",
    "            trainer_d(net_g1, net_d1, optimizer_d1, image_a, image_b, pool_a2b)\n",
    "            trainer_d(net_g2, net_d2, optimizer_d2, image_b, image_a, pool_b2a)\n",
    "            if iteration % step_g == 0:\n",
    "                net_g1.train()\n",
    "                net_g2.train()         \n",
    "                trainer_g(net_g1, net_d1, net_g2, optimizer_g1, image_a, image_b, apply_cycle_loss=True)\n",
    "                trainer_g(net_g2, net_d2, net_g1, optimizer_g2, image_b, image_a, apply_cycle_loss=True)\n",
    "    #\n",
    "        if iteration % 1000 == 0:\n",
    "            print('Iteration: ', iteration)       \n",
    "            T.save(net_d1.state_dict(), path_work + 'net_d1.pt')\n",
    "            T.save(net_d1.state_dict(), path_work + '备用net_d1.pt')\n",
    "            T.save(optimizer_d1.state_dict(), path_work + 'optimizer_d1.pt')\n",
    "            T.save(optimizer_d1.state_dict(), path_work + '备用optimizer_d1.pt')\n",
    "            T.save(net_g1.state_dict(), path_work + 'net_g1.pt')\n",
    "            T.save(net_g1.state_dict(), path_work + '备用net_g1.pt')\n",
    "            T.save(optimizer_g1.state_dict(), path_work + 'optimizer_g1.pt')\n",
    "            T.save(optimizer_g1.state_dict(), path_work + '备用optimizer_g1.pt')\n",
    "            T.save(net_d2.state_dict(), path_work + 'net_d2.pt')\n",
    "            T.save(net_d2.state_dict(), path_work + '备用net_d2.pt')\n",
    "            T.save(optimizer_d2.state_dict(), path_work + 'optimizer_d2.pt')\n",
    "            T.save(optimizer_d2.state_dict(), path_work + '备用optimizer_d2.pt')\n",
    "            T.save(net_g2.state_dict(), path_work + 'net_g2.pt')\n",
    "            T.save(net_g2.state_dict(), path_work + '备用net_g2.pt')\n",
    "            T.save(optimizer_g2.state_dict(), path_work + 'optimizer_g2.pt')\n",
    "            T.save(optimizer_g2.state_dict(), path_work + '备用optimizer_g2.pt')\n",
    "            np.save(path_work + 'iteration.npy', iteration) \n",
    "            show(net_g1, net_g2)    \n",
    "            print('These iterations cost {} seconds'.format(time.time() - time_start))\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.183103Z",
     "iopub.status.busy": "2022-06-23T11:30:11.182735Z",
     "iopub.status.idle": "2022-06-23T11:30:11.186877Z",
     "shell.execute_reply": "2022-06-23T11:30:11.186408Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.183081Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(iterations=1, load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T11:30:11.187970Z",
     "iopub.status.busy": "2022-06-23T11:30:11.187652Z",
     "iopub.status.idle": "2022-06-23T11:30:34.859284Z",
     "shell.execute_reply": "2022-06-23T11:30:34.857856Z",
     "shell.execute_reply.started": "2022-06-23T11:30:11.187949Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(iterations=iterations, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-23T11:30:34.860069Z",
     "iopub.status.idle": "2022-06-23T11:30:34.860330Z",
     "shell.execute_reply": "2022-06-23T11:30:34.860209Z",
     "shell.execute_reply.started": "2022-06-23T11:30:34.860195Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_g1 = Generator().to(device)\n",
    "net_g1.load_state_dict(T.load(path_work + 'net_g1.pt'))\n",
    "net_g2 = Generator().to(device)\n",
    "net_g2.load_state_dict(T.load(path_work + 'net_g2.pt'))\n",
    "show(net_g1, net_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cf6da5e8beced390049d9c6139c395b01a9f103347908593030ecc8eec04e13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
